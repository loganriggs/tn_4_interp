{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035ac35c",
   "metadata": {},
   "source": [
    "# Stacking Bilinear Layers\n",
    "The goal of this notebook is to get experience with stacking bilinear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a46228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import einsum\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "from typing import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# Bilinear layer similar to the other notebook (left/right via chunk)\n",
    "class Bilinear(nn.Linear):\n",
    "    def __init__(self, d_in: int, d_out: int, bias=False) -> None:\n",
    "        super().__init__(d_in, 2 * d_out, bias=bias)\n",
    "    def forward(self, x):\n",
    "        left, right = super().forward(x).chunk(2, dim=-1)\n",
    "        return left * right\n",
    "    @property\n",
    "    def w_l(self):\n",
    "        return self.weight.chunk(2, dim=0)[0]\n",
    "    @property\n",
    "    def w_r(self):\n",
    "        return self.weight.chunk(2, dim=0)[1]\n",
    "\n",
    "@dataclass\n",
    "class BiLayerConfig:\n",
    "    d_in: int\n",
    "    d_hid: int\n",
    "    d_out: int\n",
    "    bias: bool = False\n",
    "\n",
    "class BiLayer(nn.Module):\n",
    "    def __init__(self, cfg: BiLayerConfig):\n",
    "        super().__init__()\n",
    "        self.bi_linear = Bilinear(d_in=2*cfg.d_in, d_out=cfg.d_hid, bias=cfg.bias)\n",
    "        self.projection = nn.Linear(cfg.d_hid, cfg.d_out, bias=cfg.bias)\n",
    "    def forward(self, x):\n",
    "        return self.projection(self.bi_linear(x))\n",
    "    @property\n",
    "    def w_l(self):\n",
    "        return self.bi_linear.w_l\n",
    "    @property\n",
    "    def w_r(self):\n",
    "        return self.bi_linear.w_r\n",
    "    @property\n",
    "    def w_p(self):\n",
    "        return self.projection.weight\n",
    "\n",
    "@dataclass\n",
    "class BiStackConfig:\n",
    "    dims: List[int]\n",
    "    bias: bool = False\n",
    "\n",
    "class BiStack(nn.Module):\n",
    "    def __init__(self, cfg: BiStackConfig):\n",
    "        super().__init__()\n",
    "        # dims in 0, 3, 5, 7, 9, ...\n",
    "        assert len(cfg.dims) >= 3 and len(cfg.dims) % 2 == 1\n",
    "        layers = []\n",
    "        for i in range(0, len(cfg.dims) - 2, 2):  # Step by 2\n",
    "            if len(cfg.dims) - i - 2 < 0:\n",
    "                break\n",
    "            layer_cfg = BiLayerConfig(\n",
    "                d_in=cfg.dims[i], d_hid=cfg.dims[i+1], \n",
    "                d_out=cfg.dims[i+2], bias=cfg.bias\n",
    "                )\n",
    "            layers.append(BiLayer(layer_cfg))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def w_l(self, layer: int):\n",
    "        return self.layers[layer].bi_linear.w_l\n",
    "    @property\n",
    "    def w_r(self, layer: int):\n",
    "        return self.layers[layer].bi_linear.w_r\n",
    "    @property\n",
    "    def w_p(self, layer: int):\n",
    "        return self.layers[layer].projection.weight\n",
    "\n",
    "def init_stack(dims: List[int], bias: bool = False) -> BiStack:\n",
    "    cfg = BiStackConfig(dims=dims, bias=bias)\n",
    "    return BiStack(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9258d83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiStack(\n",
       "  (layers): ModuleList(\n",
       "    (0): BiLayer(\n",
       "      (bi_linear): Bilinear(in_features=12, out_features=4, bias=False)\n",
       "      (projection): Linear(in_features=2, out_features=3, bias=False)\n",
       "    )\n",
       "    (1): BiLayer(\n",
       "      (bi_linear): Bilinear(in_features=6, out_features=8, bias=False)\n",
       "      (projection): Linear(in_features=4, out_features=5, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_stack([6, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa1d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities: data generation, loaders, criterion and training loop (adapted from application.ipynb)\n",
    "import math\n",
    "from typing import Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    P: int = 113\n",
    "    train_size: float = 0.8\n",
    "\n",
    "def generate_modular_addition_data(P: int):\n",
    "    a_vals = torch.arange(P).repeat_interleave(P)\n",
    "    b_vals = torch.arange(P).repeat(P)\n",
    "    x_vals = torch.stack((a_vals, b_vals), dim=-1)\n",
    "\n",
    "    a_1hot = torch.nn.functional.one_hot(a_vals, num_classes=P).float()\n",
    "    b_1hot = torch.nn.functional.one_hot(b_vals, num_classes=P).float()\n",
    "    x_1hot = torch.cat((a_1hot, b_1hot), dim=-1)\n",
    "\n",
    "    targets = (a_vals + b_vals) % P\n",
    "    return x_vals, x_1hot, targets\n",
    "\n",
    "# Loss map and optimizer map (minimal)\n",
    "_LOSS_MAP = {\n",
    "    \"crossentropy\": nn.CrossEntropyLoss,\n",
    "}\n",
    "_OPTIMIZER_MAP = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adamw\": torch.optim.AdamW,\n",
    "    \"sgd\": torch.optim.SGD,\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class CriterionConfig:\n",
    "    name: str = \"crossentropy\"\n",
    "    kwargs: dict | None = None\n",
    "\n",
    "def get_criterion(cfg: CriterionConfig) -> nn.Module:\n",
    "    key = cfg.name.replace(\" \", \"\").replace(\"-\", \"\").lower()\n",
    "    return _LOSS_MAP[key](**(cfg.kwargs or {}))\n",
    "\n",
    "@dataclass\n",
    "class OptimizerConfig:\n",
    "    name: str = \"adamw\"\n",
    "    lr: float = 0.003\n",
    "    weight_decay: float | None = 1e-4\n",
    "\n",
    "def get_optimizer(params, config: OptimizerConfig) -> torch.optim.Optimizer:\n",
    "    key = config.name.replace(\"-\", \"\").replace(\"_\", \"\").lower()\n",
    "    cls = _OPTIMIZER_MAP[key]\n",
    "    kwargs = {\"lr\": config.lr}\n",
    "    if config.weight_decay is not None: kwargs['weight_decay'] = config.weight_decay\n",
    "    return cls(params, **kwargs)\n",
    "\n",
    "def get_loaders(x: torch.Tensor, t: torch.LongTensor, train_size: float, batch_size: int):\n",
    "    perm = torch.randperm(x.size(0))\n",
    "    x, t = x[perm], t[perm]\n",
    "    split = math.ceil(train_size * x.size(0))\n",
    "    x_train, x_valid = x.split(split)\n",
    "    t_train, t_valid = t.split(split)\n",
    "    train_data = torch.utils.data.TensorDataset(x_train, t_train)\n",
    "    valid_data = torch.utils.data.TensorDataset(x_valid, t_valid)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def eval_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for x, t in loader:\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            logit = model(x)\n",
    "            loss = criterion(logit, t)\n",
    "            losses.append(loss.item())\n",
    "            pred = logit.argmax(dim=-1)\n",
    "            accs.append((pred == t).float().mean().item())\n",
    "    return sum(losses)/len(losses), sum(accs)/len(accs)\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    batch_size: int = 256\n",
    "    epochs: int = 400\n",
    "    optimizer = OptimizerConfig\n",
    "    criterion = CriterionConfig\n",
    "\n",
    "def train_loop(model, loaders, cfg: TrainConfig):\n",
    "    criterion = get_criterion(cfg.criterion)\n",
    "    optimizer = get_optimizer(model.parameters(), cfg.optimizer)\n",
    "    train_loader, valid_loader = loaders\n",
    "    train_losses, train_accs, valid_losses, valid_accs = [], [], [], []\n",
    "    best_valid = float('inf')\n",
    "    best_state = None\n",
    "    pbar = tqdm(range(cfg.epochs), desc=\"Training\")\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        epoch_losses, epoch_accs = [], []\n",
    "        for x, t in train_loader:\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            logit = model(x)\n",
    "            loss = criterion(logit, t)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "            pred = logit.argmax(dim=-1)\n",
    "            epoch_accs.append((pred == t).float().mean().item())\n",
    "        train_losses.append(sum(epoch_losses)/len(epoch_losses))\n",
    "        train_accs.append(sum(epoch_accs)/len(epoch_accs))\n",
    "        vloss, vacc = eval_model(model, valid_loader, criterion)\n",
    "        valid_losses.append(vloss)\n",
    "        valid_accs.append(vacc)\n",
    "        if vloss < best_valid:\n",
    "            best_valid = vloss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        pbar.set_postfix({'train_loss': f'{train_losses[-1]:.4f}', 'valid_loss': f'{vloss:.4f}', 'valid_acc': f'{vacc:.4f}'})\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return {'train_losses': train_losses, 'train_accs': train_accs, 'valid_losses': valid_losses, 'valid_accs': valid_accs, 'best_valid_loss': best_valid}\n",
    "\n",
    "def plot_training_results(train_losses, valid_losses, train_accs, valid_accs, arc=\"Stack\", y_scale=\"linear\", cut_off_epoch=None):\n",
    "    if cut_off_epoch is None: cut_off_epoch = len(train_losses)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,5))\n",
    "    ax1.plot(train_losses[:cut_off_epoch], label='Train Loss')\n",
    "    ax1.plot(valid_losses[:cut_off_epoch], label='Valid Loss')\n",
    "    ax1.set_yscale(y_scale)\n",
    "    ax1.legend()\n",
    "    ax2.plot(train_accs[:cut_off_epoch], label='Train Acc')\n",
    "    ax2.plot(valid_accs[:cut_off_epoch], label='Valid Acc')\n",
    "    ax2.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b27474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment with a two-layer BiStack (dims length = 5 creates 2 BiLayers)\n",
    "P = DataConfig.P\n",
    "pairs, onehots, labels = generate_modular_addition_data(P=P)\n",
    "print(f'pairs: {pairs.size()}, onehots: {onehots.size()}, labels: {labels.size()}')\n",
    "loaders = get_loaders(onehots, labels, DataConfig.train_size, TrainConfig.batch_size)\n",
    "\n",
    "# dims = [P, hid1, mid, hid2, P] -> creates two BiLayer layers (i=0 and i=2)\n",
    "dims = [P, P, P, P, P]\n",
    "stack = init_stack(dims, bias=False).to(device)\n",
    "\n",
    "# Train\n",
    "cfg = TrainConfig()\n",
    "cfg.epochs = 400\n",
    "cfg.batch_size = 256\n",
    "results = train_loop(stack, loaders, cfg)\n",
    "\n",
    "# Plot results\n",
    "plot_training_results(results['train_losses'], results['valid_losses'], results['train_accs'], results['valid_accs'], arc=\"BiStack (2 layers)\", y_scale=\"log\", cut_off_epoch=50)\n",
    "\n",
    "# Sanity check on a single example\n",
    "test_point = 1337\n",
    "one_hot = onehots[test_point].unsqueeze(0).to(device)\n",
    "print('label:', labels[test_point].item())\n",
    "print('pred:', torch.argmax(stack(one_hot), dim=-1).item())\n",
    "total_params = sum(p.numel() for p in stack.parameters() if p.requires_grad)\n",
    "print(f'Total parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09992325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bi_interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
